{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use He initialization: [He et al., 2015. Delving Deep into Rectifiers: Surpassing Human-Level Performance in ImageNet Classification](https://www.cv-foundation.org/openaccess/content_iccv_2015/papers/He_Delving_Deep_into_ICCV_2015_paper.pdf). Initializes weights $W$ as random Normal variables with variance $\\frac{2}{\\sqrt{n^{L-1}}}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters(layer_dims):\n",
    "    \"\"\"\n",
    "    Initializes the neural network parameters.\n",
    "    \n",
    "    Args:\n",
    "    layer_dims: A list of the dimensions of every layer.\n",
    "    \n",
    "    Returns:\n",
    "    parameters: A dict object containing keys, \"W1\", \"b1\", etc.\n",
    "                Each Wi has dimensions (layer_dims[l], layer_dims[l-1])\n",
    "                Each bi has dimensions (layer_dims[l], 1)\n",
    "    \"\"\"\n",
    "    parameters = {}\n",
    "    L = len(layer_dims)\n",
    "    \n",
    "    for i in range(1, L):\n",
    "        # Use He initialization: \n",
    "        # He et al., 2015. Delving Deep into Rectifiers: Surpassing Human-Level Performance in \n",
    "        # ImageNet Classification.\n",
    "        parameters['W' + str(i)] = np.random.randn(layer_dims[i], layer_dims[i-1]) * np.sqrt(2 / layer_dims[i-1])\n",
    "        parameters['b' + str(i)] = np.zeros((layer_dims[i], 1))\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two activations are implemented:  \n",
    "* Sigmoid: $\\frac{1}{1 + e^{-z}}$\n",
    "* ReLU: $max(0, z)$\n",
    "\n",
    "The forward prop can be thought of as each layer computing $W^T X + b$, and applying an activation on the resulting vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    \"\"\"\n",
    "    Returns the sigmoid of the vector z\n",
    "    \n",
    "    Args:\n",
    "    z: The vector to take sigmoid of\n",
    "    \n",
    "    Returns: The softmax of the values and the cache, z\n",
    "    \"\"\"\n",
    "    sigmoid = 1 / (1 + np.exp(-z))\n",
    "    return (sigmoid, z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(z):\n",
    "    \"\"\"\n",
    "    Returns a ReLu activation of the vector z\n",
    "    \n",
    "    Args:\n",
    "    z: The vector to take relu of\n",
    "    \n",
    "    Returns:\n",
    "    a: The ReLu of z\n",
    "    cache: The value z\n",
    "    \"\"\"\n",
    "    out = np.maximum(z, 0)\n",
    "    return out, z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_forward(A, W, b):\n",
    "    \"\"\"\n",
    "    Implement the first part of a layer, i.e., computing the weighted sums.\n",
    "    \n",
    "    Args:\n",
    "    A: Activations from the previous layer\n",
    "    W: Weight matrix for this layer\n",
    "    b: Bias vector for this layer\n",
    "    \n",
    "    Returns:\n",
    "    Z: The weighted sum vector\n",
    "    cache: A tuple containing \"A\", \"W\", and \"b\" for use in backward prop\n",
    "    \"\"\"\n",
    "    Z = np.dot(W, A) + b\n",
    "    cache = (A, W, b)\n",
    "    \n",
    "    return Z, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_layer_forward(A_prev, W, b, activation):\n",
    "    \"\"\"\n",
    "    Implements a single layer's forward prop, with activation.\n",
    "    \n",
    "    Args:\n",
    "    A_prev: Activations from the previous layer\n",
    "    W: The weight matrix of the current layer\n",
    "    b: The bias vector for the current layer\n",
    "    activation: A string, 'relu' or 'sigmoid'\n",
    "    \n",
    "    Returns:\n",
    "    A: The outputs of the current layer's activations\n",
    "    cache: A tuple containing 'linear_cache' and 'activation_cache'\n",
    "    \"\"\"\n",
    "    Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "    \n",
    "    if activation == 'sigmoid':\n",
    "        A, activation_cache = sigmoid(Z)\n",
    "    elif activation == 'relu':\n",
    "        A, activation_cache = relu(Z)\n",
    "    \n",
    "    cache = (linear_cache, activation_cache)\n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagate(X, parameters, activations):\n",
    "    \"\"\"\n",
    "    Implements forward propagation.\n",
    "    \n",
    "    Args:\n",
    "    X: The inputs\n",
    "    parameters: The parameters initialized\n",
    "    activations: The activations to apply at each layer\n",
    "    \n",
    "    Returns:\n",
    "    AL: The final outputs\n",
    "    caches: A dict with the caches from each layer\n",
    "    \"\"\"\n",
    "    A = X\n",
    "    caches = []\n",
    "    L = len(parameters) // 2\n",
    "    \n",
    "    for i in range(1, L):\n",
    "        A_prev = A\n",
    "        \n",
    "        A, cache = single_layer_forward(A_prev, \n",
    "                                        W=parameters['W' + str(i)], \n",
    "                                        b=parameters['b' + str(i)],\n",
    "                                        activation=activations[i-1])\n",
    "        caches.append(cache)\n",
    "    \n",
    "    AL, cache = single_layer_forward(A, \n",
    "                                     W=parameters['W' + str(L)], \n",
    "                                     b=parameters['b' + str(L)], \n",
    "                                     activation='sigmoid')\n",
    "    caches.append(cache)\n",
    "    return AL, caches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cross-entropy cost function is $-\\frac{1}{m} \\sum \\limits_{i=1}^m \\left( y^{(i)}\\log a^{[L](i)} + (1 - y^{(i)})\\log (1 - a^{[L](i)}) \\right)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(AL, Y):\n",
    "    \"\"\"\n",
    "    Computes the cross-entropy loss\n",
    "    \n",
    "    Args:\n",
    "    AL: Sigmoid output of the network\n",
    "    Y: True outputs\n",
    "    \n",
    "    Returns:\n",
    "    cost: The current cost\n",
    "    \"\"\"\n",
    "    m = max(*Y.shape)\n",
    "    Y = Y.reshape(1, m)\n",
    "    \n",
    "    cost = (-1. / m) * np.sum(np.multiply(Y, np.log(AL)) + np.multiply((1-Y), np.log(1-AL)))\n",
    "    cost = np.squeeze(cost)\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backward prop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the linear part, the formulas are:  \n",
    "\n",
    "$Z^{[l]} = W^{[l]T} A^{[l-1]} + b^{[l]}$  \n",
    "\n",
    "1. **Weight matrix:**   \n",
    "$\\frac{\\mathrm{d}L}{\\mathrm{d}W^{[l]}} = \\frac{1}{m} \\frac{\\mathrm{d}L}{\\mathrm{d}Z^{[l]}}A^{[l-1]T}$\n",
    "\n",
    "2. **Bias term:**  \n",
    "$\\frac{\\mathrm{d}L}{\\mathrm{d}b^{[l]}} = \\frac{1}{m} \\sum \\limits_{i=1}^m \\frac{\\mathrm{d}L}{\\mathrm{d}Z^{[l](i)}}$\n",
    "\n",
    "3. **Previous layer activation:**  \n",
    "$\\frac{\\mathrm{d}L}{\\mathrm{d}A^{[l-1]}} = W^{[l]T} \\frac{\\mathrm{d}L}{\\mathrm{d}Z^{[l]}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_backward(dZ, cache):\n",
    "    \"\"\"\n",
    "    Implements the linear part of the backward propagation for one layer\n",
    "    \n",
    "    Args:\n",
    "    dZ: The derivative of the cost wrt linear output of current layer\n",
    "    cache: The cache from the forward propagation\n",
    "    \n",
    "    Returns:\n",
    "    dA_prev: Derivative of cost wrt A[l-1]\n",
    "    dW: Derivative of cost wrt W[l]\n",
    "    db: Derivative of cost wrt b\n",
    "    \"\"\"\n",
    "    A_prev, W, b = cache\n",
    "    m = A_prev.shape[1]\n",
    "    \n",
    "    dW = 1. / m * np.dot(dZ, A_prev.T)\n",
    "    db = 1. / m * np.sum(dZ, axis=1, keepdims=True)\n",
    "    dA_prev = np.dot(W.T, dZ)\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, implement the backward prop for the whole layer including the activation.  \n",
    "$A^{[l]} = g(Z^{[l]})$  \n",
    "$dZ^{[l]} = dA^{[l]} \\times g^{\\prime}(Z^{[l]})$ (why?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu_backward(dA, cache):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for a single RELU unit.\n",
    "    \n",
    "    Args:\n",
    "    dA: Post-activation gradient\n",
    "    cache: \"Z\" stored for computing gradient\n",
    "    \n",
    "    Returns:\n",
    "    dZ: Gradient of the cost wrt z\n",
    "    \"\"\"\n",
    "    Z = cache\n",
    "    dZ = np.array(dA, copy=True)\n",
    "    \n",
    "    dZ[Z <= 0] = 0\n",
    "    \n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_backward(dA, cache, activation):\n",
    "    \"\"\"\n",
    "    Implements the backward pass for one complete layer, including activations.\n",
    "    \n",
    "    Args:\n",
    "    dA: Gradient of activation of the current layer\n",
    "    cache: Tuple (linear_cache, activation_cache) from forward pass\n",
    "    activation: The activation at the current layer\n",
    "    \n",
    "    Returns:\n",
    "    dA_prev: Gradient of loss wrt A[l-1]\n",
    "    dW: Gradient of loss wrt W[l]\n",
    "    db: Gradient of loss wrt b[l]\n",
    "    \"\"\"\n",
    "    linear_cache, activation_cache = cache\n",
    "    \n",
    "    if activation == 'relu':\n",
    "        dZ = relu_backward(dA, activation_cache)\n",
    "    elif activation == 'sigmoid':\n",
    "        dZ = dA * sigmoid(activation_cache)[0] * (1 - sigmoid(activation_cache)[0])\n",
    "    \n",
    "    dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_prop(AL, Y, caches, activations):\n",
    "    \"\"\"\n",
    "    Implements backward propagation.\n",
    "    \n",
    "    Args:\n",
    "    AL: The output of the network\n",
    "    Y: The true output\n",
    "    caches: The caches collected during forward prop\n",
    "    \n",
    "    Returns:\n",
    "    grads: A dict with gradients grads['dW1'], grads['dA1'], grads['db1'], etc.\n",
    "    \"\"\"\n",
    "    grads = {}\n",
    "    L = len(caches)\n",
    "    m = AL.shape[1]\n",
    "    Y = Y.reshape(AL.shape)\n",
    "    \n",
    "    dAL = -(Y / AL - (1 - Y) / (1 - AL))\n",
    "    \n",
    "    cache = caches[-1]\n",
    "    grads['dA' + str(L)], grads['dW' + str(L)], grads['db' + str(L)] = single_backward(dAL, cache, activations[-1])\n",
    "\n",
    "    for i in reversed(range(L-1)):\n",
    "        cache = caches[i]\n",
    "        \n",
    "        dA_prev, dW, db = single_backward(grads['dA' + str(i+2)], cache, activations[i])\n",
    "        \n",
    "        grads['dA' + str(i+1)] = dA_prev\n",
    "        grads['dW' + str(i+1)] = dW\n",
    "        grads['db' + str(i+1)] = db\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(params, grads, learning_rate):\n",
    "    \"\"\"\n",
    "    Updates parameters using gradient descent\n",
    "    \n",
    "    Args:\n",
    "    params: The current parameters of the model\n",
    "    grads: The gradients from backward pass\n",
    "    learning_rate: The learning rate for GD\n",
    "    \n",
    "    Returns:\n",
    "    parameters: Parameters after gradient descent\n",
    "    \"\"\"\n",
    "    L = len(parameters) // 2\n",
    "    \n",
    "    for i in range(L):\n",
    "        parameters['W' + str(i+1)] -= learning_rate * grads['dW' + str(i+1)]\n",
    "        parameters['b' + str(i+1)] -= learning_rate * grads['db' + str(i+1)]\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The full model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = initialize_parameters([2, 2, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "grads = {}\n",
    "costs = []\n",
    "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]]).T\n",
    "Y = np.array([0, 1, 1, 0]).reshape(4, 1)\n",
    "m = X.shape[1]\n",
    "n_x, n_h, n_y = (2, 2, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "W1 = parameters['W1']\n",
    "b1 = parameters['b1']\n",
    "W2 = parameters['W2']\n",
    "b2 = parameters['b2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1000):\n",
    "    A3, caches = forward_propagate(X, parameters, ['relu', 'relu', 'sigmoid'])\n",
    "    cost = compute_cost(A3, Y)\n",
    "    grads = backward_prop(A3, Y, caches, ['relu', 'relu', 'sigmoid'])\n",
    "    parameters = update_parameters(parameters, grads, 0.01)\n",
    "    costs.append(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(costs);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
