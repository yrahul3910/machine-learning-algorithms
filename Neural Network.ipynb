{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use He initialization: [He et al., 2015. Delving Deep into Rectifiers: Surpassing Human-Level Performance in ImageNet Classification](https://www.cv-foundation.org/openaccess/content_iccv_2015/papers/He_Delving_Deep_into_ICCV_2015_paper.pdf). Initializes weights $W$ as random Normal variables with variance $\\frac{2}{\\sqrt{n^{L-1}}}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters(layer_dims):\n",
    "    \"\"\"\n",
    "    Initializes the neural network parameters.\n",
    "    \n",
    "    Args:\n",
    "    layer_dims: A list of the dimensions of every layer.\n",
    "    \n",
    "    Returns:\n",
    "    parameters: A dict object containing keys, \"W1\", \"b1\", etc.\n",
    "                Each Wi has dimensions (layer_dims[l], layer_dims[l-1])\n",
    "                Each bi has dimensions (layer_dims[l], 1)\n",
    "    \"\"\"\n",
    "    parameters = {}\n",
    "    L = len(layer_dims)\n",
    "    \n",
    "    for i in range(1, L):\n",
    "        # Use He initialization: \n",
    "        # He et al., 2015. Delving Deep into Rectifiers: Surpassing Human-Level Performance in \n",
    "        # ImageNet Classification.\n",
    "        parameters['W' + str(i)] = np.random.randn(layer_dims[i], layer_dims[i-1]) * np.sqrt(2 / layer_dims[i-1])\n",
    "        parameters['b' + str(i)] = np.zeros((layer_dims[i], 1))\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two activations are implemented:  \n",
    "* Sigmoid: $\\frac{1}{1 + e^{-z}}$\n",
    "* ReLU: $max(0, z)$\n",
    "\n",
    "The forward prop can be thought of as each layer computing $W^T X + b$, and applying an activation on the resulting vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    \"\"\"\n",
    "    Returns the sigmoid of the vector z\n",
    "    \n",
    "    Args:\n",
    "    z: The vector to take sigmoid of\n",
    "    \n",
    "    Returns: The softmax of the values and the cache, z\n",
    "    \"\"\"\n",
    "    sigmoid = 1 / (1 + np.exp(-z))\n",
    "    return (sigmoid, z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(z):\n",
    "    \"\"\"\n",
    "    Returns a ReLu activation of the vector z\n",
    "    \n",
    "    Args:\n",
    "    z: The vector to take relu of\n",
    "    \n",
    "    Returns:\n",
    "    a: The ReLu of z\n",
    "    cache: The value z\n",
    "    \"\"\"\n",
    "    out = np.maximum(z, 0)\n",
    "    return out, z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_forward(A, W, b):\n",
    "    \"\"\"\n",
    "    Implement the first part of a layer, i.e., computing the weighted sums.\n",
    "    \n",
    "    Args:\n",
    "    A: Activations from the previous layer\n",
    "    W: Weight matrix for this layer\n",
    "    b: Bias vector for this layer\n",
    "    \n",
    "    Returns:\n",
    "    Z: The weighted sum vector\n",
    "    cache: A tuple containing \"A\", \"W\", and \"b\" for use in backward prop\n",
    "    \"\"\"\n",
    "    Z = np.dot(W, A) + b\n",
    "    cache = (A, W, b)\n",
    "    \n",
    "    return Z, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_layer_forward(A_prev, W, b, activation):\n",
    "    \"\"\"\n",
    "    Implements a single layer's forward prop, with activation.\n",
    "    \n",
    "    Args:\n",
    "    A_prev: Activations from the previous layer\n",
    "    W: The weight matrix of the current layer\n",
    "    b: The bias vector for the current layer\n",
    "    activation: A string, 'relu' or 'sigmoid'\n",
    "    \n",
    "    Returns:\n",
    "    A: The outputs of the current layer's activations\n",
    "    cache: A tuple containing 'linear_cache' and 'activation_cache'\n",
    "    \"\"\"\n",
    "    Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "    \n",
    "    if activation == 'sigmoid':\n",
    "        A, activation_cache = sigmoid(Z)\n",
    "    elif activation == 'relu':\n",
    "        A, activation_cache = relu(Z)\n",
    "    \n",
    "    cache = (linear_cache, activation_cache)\n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagate(X, parameters, activations):\n",
    "    \"\"\"\n",
    "    Implements forward propagation.\n",
    "    \n",
    "    Args:\n",
    "    X: The inputs\n",
    "    parameters: The parameters initialized\n",
    "    activations: The activations to apply at each layer\n",
    "    \n",
    "    Returns:\n",
    "    AL: The final outputs\n",
    "    caches: A dict with the caches from each layer\n",
    "    \"\"\"\n",
    "    A = X\n",
    "    caches = []\n",
    "    L = len(parameters) // 2\n",
    "    \n",
    "    for i in range(1, L):\n",
    "        A_prev = A\n",
    "        \n",
    "        A, cache = single_layer_forward(A_prev, \n",
    "                                        W=parameters['W' + str(i)], \n",
    "                                        b=parameters['b' + str(i)],\n",
    "                                        activation=activations[i-1])\n",
    "        caches.append(cache)\n",
    "    \n",
    "    AL, cache = single_layer_forward(A, \n",
    "                                     W=parameters['W' + str(L)], \n",
    "                                     b=parameters['b' + str(L)], \n",
    "                                     activation='sigmoid')\n",
    "    caches.append(cache)\n",
    "    return AL, caches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cross-entropy cost function is $-\\frac{1}{m} \\sum \\limits_{i=1}^m \\left( y^{(i)}\\log a^{[L](i)} + (1 - y^{(i)})\\log (1 - a^{[L](i)}) \\right)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(AL, Y):\n",
    "    \"\"\"\n",
    "    Computes the cross-entropy loss\n",
    "    \n",
    "    Args:\n",
    "    AL: Sigmoid output of the network\n",
    "    Y: True outputs\n",
    "    \n",
    "    Returns:\n",
    "    cost: The current cost\n",
    "    \"\"\"\n",
    "    m = max(*Y.shape)\n",
    "    Y = Y.reshape(1, m)\n",
    "    \n",
    "    cost = (-1. / m) * np.sum(np.multiply(Y, np.log(AL)) + np.multiply((1-Y), np.log(1-AL)))\n",
    "    cost = np.squeeze(cost)\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backward prop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the linear part, the formulas are:  \n",
    "\n",
    "$Z^{[l]} = W^{[l]T} A^{[l-1]} + b^{[l]}$  \n",
    "\n",
    "1. **Weight matrix:**   \n",
    "$\\frac{\\mathrm{d}L}{\\mathrm{d}W^{[l]}} = \\frac{1}{m} \\frac{\\mathrm{d}L}{\\mathrm{d}Z^{[l]}}A^{[l-1]T}$\n",
    "\n",
    "2. **Bias term:**  \n",
    "$\\frac{\\mathrm{d}L}{\\mathrm{d}b^{[l]}} = \\frac{1}{m} \\sum \\limits_{i=1}^m \\frac{\\mathrm{d}L}{\\mathrm{d}Z^{[l](i)}}$\n",
    "\n",
    "3. **Previous layer activation:**  \n",
    "$\\frac{\\mathrm{d}L}{\\mathrm{d}A^{[l-1]}} = W^{[l]T} \\frac{\\mathrm{d}L}{\\mathrm{d}Z^{[l]}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_backward(dZ, cache):\n",
    "    \"\"\"\n",
    "    Implements the linear part of the backward propagation for one layer\n",
    "    \n",
    "    Args:\n",
    "    dZ: The derivative of the cost wrt linear output of current layer\n",
    "    cache: The cache from the forward propagation\n",
    "    \n",
    "    Returns:\n",
    "    dA_prev: Derivative of cost wrt A[l-1]\n",
    "    dW: Derivative of cost wrt W[l]\n",
    "    db: Derivative of cost wrt b\n",
    "    \"\"\"\n",
    "    A_prev, W, b = cache\n",
    "    m = A_prev.shape[1]\n",
    "    \n",
    "    dW = 1. / m * np.dot(dZ, A_prev.T)\n",
    "    db = 1. / m * np.sum(dZ, axis=1, keepdims=True)\n",
    "    dA_prev = np.dot(W.T, dZ)\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, implement the backward prop for the whole layer including the activation.  \n",
    "$A^{[l]} = g(Z^{[l]})$  \n",
    "$dZ^{[l]} = dA^{[l]} \\times g^{\\prime}(Z^{[l]})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu_backward(dA, cache):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for a single RELU unit.\n",
    "    \n",
    "    Args:\n",
    "    dA: Post-activation gradient\n",
    "    cache: \"Z\" stored for computing gradient\n",
    "    \n",
    "    Returns:\n",
    "    dZ: Gradient of the cost wrt z\n",
    "    \"\"\"\n",
    "    Z = cache\n",
    "    dZ = np.array(dA, copy=True)\n",
    "    \n",
    "    dZ[Z <= 0] = 0\n",
    "    \n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_backward(dA, cache, activation):\n",
    "    \"\"\"\n",
    "    Implements the backward pass for one complete layer, including activations.\n",
    "    \n",
    "    Args:\n",
    "    dA: Gradient of activation of the current layer\n",
    "    cache: Tuple (linear_cache, activation_cache) from forward pass\n",
    "    activation: The activation at the current layer\n",
    "    \n",
    "    Returns:\n",
    "    dA_prev: Gradient of loss wrt A[l-1]\n",
    "    dW: Gradient of loss wrt W[l]\n",
    "    db: Gradient of loss wrt b[l]\n",
    "    \"\"\"\n",
    "    linear_cache, activation_cache = cache\n",
    "    \n",
    "    if activation == 'relu':\n",
    "        dZ = relu_backward(dA, activation_cache)\n",
    "    elif activation == 'sigmoid':\n",
    "        dZ = dA * sigmoid(activation_cache)[0] * (1 - sigmoid(activation_cache)[0])\n",
    "    \n",
    "    dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_prop(AL, Y, caches, activations):\n",
    "    \"\"\"\n",
    "    Implements backward propagation.\n",
    "    \n",
    "    Args:\n",
    "    AL: The output of the network\n",
    "    Y: The true output\n",
    "    caches: The caches collected during forward prop\n",
    "    \n",
    "    Returns:\n",
    "    grads: A dict with gradients grads['dW1'], grads['dA1'], grads['db1'], etc.\n",
    "    \"\"\"\n",
    "    grads = {}\n",
    "    L = len(caches)\n",
    "    m = AL.shape[1]\n",
    "    Y = Y.reshape(AL.shape)\n",
    "    \n",
    "    dAL = -(Y / AL - (1 - Y) / (1 - AL))\n",
    "    \n",
    "    cache = caches[-1]\n",
    "    grads['dA' + str(L)], grads['dW' + str(L)], grads['db' + str(L)] = single_backward(dAL, cache, activations[-1])\n",
    "\n",
    "    for i in reversed(range(L-1)):\n",
    "        cache = caches[i]\n",
    "        \n",
    "        dA_prev, dW, db = single_backward(grads['dA' + str(i+2)], cache, activations[i])\n",
    "        \n",
    "        grads['dA' + str(i+1)] = dA_prev\n",
    "        grads['dW' + str(i+1)] = dW\n",
    "        grads['db' + str(i+1)] = db\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(params, grads, learning_rate):\n",
    "    \"\"\"\n",
    "    Updates parameters using gradient descent\n",
    "    \n",
    "    Args:\n",
    "    params: The current parameters of the model\n",
    "    grads: The gradients from backward pass\n",
    "    learning_rate: The learning rate for GD\n",
    "    \n",
    "    Returns:\n",
    "    parameters: Parameters after gradient descent\n",
    "    \"\"\"\n",
    "    L = len(parameters) // 2\n",
    "    \n",
    "    for i in range(L):\n",
    "        parameters['W' + str(i+1)] -= learning_rate * grads['dW' + str(i+1)]\n",
    "        parameters['b' + str(i+1)] -= learning_rate * grads['db' + str(i+1)]\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The full model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = initialize_parameters([7, 2, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7, 199)\n",
      "(1, 199)\n"
     ]
    }
   ],
   "source": [
    "grads = {}\n",
    "costs = []\n",
    "\n",
    "df = pd.read_csv('Data4.csv', header=None)\n",
    "X = np.array(df[df.columns[:-1]]).T\n",
    "Y = np.array(df[df.columns[-1]]).reshape(199, 1).T\n",
    "print(X.shape)\n",
    "print(Y.shape)\n",
    "\n",
    "m = X.shape[1]\n",
    "n_x, n_h, n_y = (7, 2, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "W1 = parameters['W1']\n",
    "b1 = parameters['b1']\n",
    "W2 = parameters['W2']\n",
    "b2 = parameters['b2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1000):\n",
    "    A3, caches = forward_propagate(X, parameters, ['relu', 'relu', 'sigmoid'])\n",
    "    cost = compute_cost(A3, Y)\n",
    "    grads = backward_prop(A3, Y, caches, ['relu', 'relu', 'sigmoid'])\n",
    "    parameters = update_parameters(parameters, grads, 0.01)\n",
    "    costs.append(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(costs);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
